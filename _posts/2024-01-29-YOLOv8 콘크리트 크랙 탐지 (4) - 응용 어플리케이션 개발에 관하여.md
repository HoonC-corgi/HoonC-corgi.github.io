---
published: True
---

# **YOLOv8 Concrete Crack Detection using Instance Segmentation #4**
## - About Developing application

-----

## Review

[이전 게시글](https://hoonc-corgi.github.io/2023/11/24/YOLOv8-콘크리트-크랙-탐지-(3)-모델-학습에-관하여.html)  
시리즈 게시글 1, 2, 3까지의 여정을 통해 건축물의 균열을 검출하도록 하는 YOLO 모델을 구축하였다.  
또한 초기에 목표인 본 모델을 HoloLens2에서의 구동할 수 있게 하기 위하여 다양한 방법과 시도를 하였으며, 본 게시글부터는 그에 관한 이야기를 풀어갈 것이다.

---

## 🚀 Inspiration
![HoloLens2](https://github.com/HoonC-corgi/Convolution_Filter_Application/assets/118245330/b77c4069-9b51-49e1-bc04-dcb6a2aa7609)
처음 구상했던 아이디어는 AR 기기인 HoloLens2(이하 홀로렌즈)를 착용하고 건물 외벽을 바라보면 모델이 균열을 검출하고, 그 길이나 폭이 측정될 수 있도록 구현하고자 했다.  
이를 위해서는 홀로렌즈에서 AR이 어떠한 매커니즘으로 동작하는지를 분명히 알아야 하는데, 이를 위해서는 홀로렌즈의 하드웨어 스펙에 대한 이해가 필요하다.  
우리가 흔히 혼합현실이라 부르는 기능의 핵심은 '**플로팅**'(위 사진과 같이 현실 공간 위에 2d 또는 3d 그래픽을 띄우는 것으로 정의하겠다) 기능이 핵심이다.


![spatial mapping mesh](https://github.com/Hyung-Gunny/Java-teamproject2-ToDoList/assets/118245330/b6b9f69a-2a1f-4127-8180-f3e60e8d250e)
또 플로팅이 구현되기 위해서는 '**Spatial Mapping**'이 필수적이며(그래픽을 현실 공간 어디에 띄워야 할 지에 대한 정보가 필요하기 때문이다), 이를 위해서는 '**LiDAR**' **Sensor**가 필요하다.
LiDAR는 "Light Detection and Ranging"의 약어로, 레이저 펄스의 반사광을 통해 거리, 공간 왜곡 등을 확인할 수 있도록하는 개념이다.
홀로렌즈 역시 적외선 광원과 인식 센서, depth 카메라 등을 통해 홀로렌즈를 착용한 사용자를 중심으로 주변 공간을 매핑하고 인식한 후 해당 공간 정보에 따라 홀로그램을 매핑하는 방식으로 구동된다.

또한 이런 하드웨어 데이터를 얻어오고 그래픽으로 표현하기 위해서는 홀로렌즈 앱 배포가 가능한 유니티 엔진과, 홀로렌즈의 백엔드 메소드 등을 사용하여야 했지만 유니티와 C# 모두 해보지 않은 나로서는 시도조차 어려운 일이었다.
구현부터 시작하고 만들어가는 것은 불가능하다고 판단하여서, 구현하고자 했던 것들과 그에 따라 필요한 것이 무엇이 있는지에 대한 계획을 시작하였다.

#### Goal
1. 홀로렌즈를 착용한 상태에서 균열 검출
2. 검출된 균열에 대한 길이와 폭 측정
3. 균열에 대한 마스킹 
4. 마스킹한 균열의 영구성 보장
5. 균열의 깊이 측정

위 네 개의 사항이 처음 생각했던 목표였으며, 이를 구현하기 위한 요구사항은 아래와 같았다.

#### Requirements && Problems
1.1. 모델이 탑재된 홀로렌즈 앱 배포
1.1.1. 앱은 홀로렌즈 하드웨어 데이터를 적절히 활용하여야 함.
1.2. 모델은 홀로렌즈의 하드웨어 스펙과 모델 입력 구조의 충돌이 없어야 함.

2.1. 길이와 폭 측정에 따른 구현 원리 선정이 필요함.  
2.1.1. 삼각측량법
2.1.2. 픽셀 비율을 통한 계산
2.1.3. Bounding Box의 크기를 통한 계산

3.1. 공간 정보가 필요함.  

4.1. 데이터 베이스 활용
4.2. 앱에 저장, 업데이트하는 구조

5.1. 균열의 영역 내에서 최대 depth 값을 가지는 지점과 최소 depth값을 가지는 지점을 알아야 함.


다섯 가지 목표에 대한 요구사항들이 본질적으로 가지는 것은 모두 홀로렌즈의 하드웨어 데이터를 자유롭게 활용할 수 있도록 하는 것이 가장 컸다.   


따라서 계획부터 수립하기 위해 할 수 있는 것은 균열의 길이와 폭을 측정하기 위한 원리를 선정해 주는 것이었다.
나는 일반적으로 거리 측정에 사용하는 삼각 측량법을 가장 먼저 떠올릴 수 있었지만, 이 역시 홀로렌즈의 depth 데이터를 활용하여야 가능한 것이기에 바로 해볼 수 있는 것이 없었다.  


![아이디어 구상](https://github.com/Hyung-Gunny/Java-teamproject2-ToDoList/assets/118245330/40164add-766c-4bdb-929d-c90bc573de61)
그렇게 위 사진과 같이 엎고 다시 생각해 보며 2.1.2.의 방법을 떠올리게 되었는데, 그 과정은 아래와 같다.
1. 500mm 크기의 균열을 가정하여 그림을 준비한다.
2. 벽에 준비한 그림을 붙인 후 1m 거리에서 균열을 바라보고 홀로렌즈로 캡처한다.
3. 홀로렌즈 캡처 이미지를 통해 한 픽셀당 실측 비율 **'ppm'**(pixel per milli meter)을 계산한다.
4. x, y 축에 대하여 최대 크기를 가지는 연속되는 픽셀의 합을 ppm 산출식에 따라 계산하여 길이와 폭을 측정한다.
5. 1m 거리에서 실제 균열을 바라보고 캡처한다.
5. 균열의 길이와 폭을 측정한다.

좋은 접근 방법이라고 생각했지만 이 방식에는 가장 큰 문제가 있었다. 실제 현장을 가정한다면, 정확히 1m를 맞추고 균열과 시선을 수평으로 맞추어서 쳐다볼 수 없다는 것이었다.  
모델을 홀로렌즈에서 구동하기를 원하는 것은 균열 검출 시의 '핸즈 프리', 즉 사용성을 높이기 위함인데 이 경우에는 손만 편하자고 사용이 안되는 경우를 만들어 버리는 노릇이었다.

![레퍼런스한 논문](https://github.com/Hyung-Gunny/Java-teamproject2-ToDoList/assets/118245330/cb35a489-5135-4dde-beda-cd40ad242a18)
거리의 경우 최초 실행 시 1회 보정 이후 LiDAR Sensor를 통한 자동 보정을 구현할 수 있겠으나, 모든 각도에 대해서 ppm을 구하는 것은 홀로렌즈의 하드웨어를 자유롭게 활용하더라도 불가능할 것이라 느껴졌다(위 사진과 같이 여러 논문들도 찾아보며 일주일을 머리를 싸매고 고민했던 것 같다).

당시에도 많이 힘들어 하면서 보낸 과정이었고, 정말 많은 시간을 투자하고도 큰 소득은 없어 많이 좌절했던 시기였던 것 같다.

결국 홀로렌즈를 통해 소규모 단위 기능으로 하나씩 구현해 보기 위해 여러 깃허브와, 홀로렌즈 공식문서 등을 하루에 9시간 가까이 쳐다보며 보내기를 3주를 반복했지만, 다섯 가지 Goal 중에서 처음 생각했던 대로 구현할 수 있는 건 아무것도 없다는 판단이 섰다. ~~눈물이 났다, 그것도 많이..~~

2달 넘게 진행한 프로젝트였기에 이대로 포기할 수는 없었고, 나는 큰 줄기들을 과감히 건드리며 절충안을 마련하였다. 어떻게든 이 프로젝트의 끝을 보고 싶었다.

#### Solutions

1. ~~홀로렌즈를 착용한 상태에서 균열 검출~~ --> 홀로렌즈 비디오 스트림으로 모바일 또는 pc에서 균열 검출  
기존에 홀로렌즈를 착용한 상태에서 균열을 검출한다는 것은 온전히 홀로렌즈 하나만으로 앱에 탑재된 모델을 통해 실시간으로 균열을 검출하고 AR로 볼 수 있도록 하는 것이었다. 즉, 핸즈 프리의 장점과 더불어 개인 작업을 용이하게 하고자 하였다.  
바뀐 목표에서는 핸즈 프리의 장점은 유지하되 사용자가 홀로렌즈를 통해 균열을 검출하게 되면, 홀로렌즈와 통신하는 모바일, pc 등에서 이를 확인할 수 있도록 하여 협업에 용이하게 하고자 하였다. 또한 이렇게 한 번 검출된 균열은 그 마스크를 오브젝트로 생성하여 해당 위치에 픽스하는 방식으로 추후에 점검 및 비교, 관리할 수 있도록 하였다.


2. ~~검출된 균열에 대한 길이와 폭 측정~~ --> 비디오 스트림을 수신하는 pc/모바일 사이드에서 균열 길이와 폭 측정
이는 그대로 가져가되, 홀로렌즈 하드웨어 데이터를 이용하지 않고, 생성되는 Bounding Box의 크기를 통해 ppm 산출식에 적용하여 계산하되, 각 측정 거리에 대한 보정까지는 최후에 구현해 보기로 하였다.


3. ~~균열에 대한 마스킹~~ --> 비디오 스트림을 수신하는 pc/모바일 사이드에서 마스킹 처리


4. 마스킹한 균열에 대한 영구성 보장
오브젝트 파일로 저장, 유니티로 앱으로 매핑하는 방식 이용, 이부분의 경우 조력을 받았다.


5. ~~균열의 깊이 측정~~


혹자가 "그렇게 할 거면 홀로렌즈 뭐하러 쓰냐?"라고 묻는다면 사실 할 말이 없다. 하지만 현재 할 수 있는 수준으로 목표를 다시 잡고, 달려가는 것이 프로젝트의 끝을 보는 것에 더 도움이 될 것이라 판단하였다, 중간에 포기하는 것보다는 무엇이든 만들어 내는 편이 좋을 것 같아서.

변경된 목표에 따라 새로 2개의 구조를 구상하였는데, 이는 아래와 같다. 

1. 양방향 통신으로, 홀로렌즈의 비디오 프레임을 PC / Mobile에서 모델에 입력하여 처리하고, 이를 다시 홀로렌즈로 송신하여 2D 이미지로 띄우는 것

<table>
  <tr>
    <td>PC / Mobile</td>
    <td>TCP</td>
    <td rowspan="2">  <--- <br> Video Stream <br> ---> </td>
    <td>TCP</td>
    <td>HoloLens2</td>
  </tr>
  <tr>
    <td>모델에 입력</td>
    <td colspan="2">모델의 출력 결과 송신</td>
    <td>모델의 출력 결과 수신</td>
    <td>검출된 균열 이미지 표시</td>
  </tr>
</table>

2. 단방향 통신으로, 홀로렌즈 비디오 프레임을 PC/ Mobile에서 모델에 입력하여 처리하고, PC / Mobile에서 확인하는 것

<table>
  <tr>
    <td>PC / Mobile</td>
    <td>TCP</td>
    <td rowspan="2">  <--- <br> Video Stream <br> </td>
    <td>TCP</td>
    <td>HoloLens2</td>
  </tr>
  <tr>
    <td>모델에 입력</td>
    <td colspan="2">모델의 출력 결과 송신</td>
    <td>모델의 출력 결과 수신</td>
    <td>검출된 균열 이미지 표시</td>
  </tr>
</table>