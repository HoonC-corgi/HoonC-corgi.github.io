---
published: True
use_math: true
---
# 딥러닝과 자연어처리 기말범위 타이핑

## Index


<!-- TOC -->
* [딥러닝과 자연어처리 기말범위 타이핑](#딥러닝과-자연어처리-기말범위-타이핑)
  * [Index](#index)
  * [타이핑을 시작하며](#타이핑을-시작하며)
  * [RNN을 이용한 텍스트 분류](#rnn을-이용한-텍스트-분류)
    * [RNN에서의 다 대 일 문제(many to one)](#rnn에서의-다-대-일-문제many-to-one)
    * [RNN에서의 다 대 다 문제(many to many)](#rnn에서의-다-대-다-문제many-to-many)
    * [RNN으로의 분류](#rnn으로의-분류)
      * [나이브 베이즈 분류기](#나이브-베이즈-분류기)
      * [이진 분류](#이진-분류-)
        * [스팸 메일 분류](#스팸-메일-분류)
        * [IMDB 리뷰 감성 분류](#imdb-리뷰-감성-분류)
        * [네이버 영화 리뷰 감성 분류](#네이버-영화-리뷰-감성-분류)
        * [BiLSTM으로 한국어 스팀 리뷰 감성 분류](#bilstm으로-한국어-스팀-리뷰-감성-분류)
      * [다중 클래스 문제](#다중-클래스-문제-)
        * [로이터 뉴스 분류](#로이터-뉴스-분류)
  * [자연어 처리를 위한 합성곱 신경망](#자연어-처리를-위한-합성곱-신경망)
    * [CNN](#cnn)
      * [합성곱 연산](#합성곱-연산)
        * [stride](#stride)
        * [가중치와 바이어스](#가중치와-바이어스)
        * [특성 맵의 크기](#특성-맵의-크기-)
        * [다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)](#다수의-채널을-가질-경우의-합성곱-연산3차원-텐서의-합성곱-연산)
        * [풀링(Pooling)](#풀링pooling)
        * [2D 합성곱](#2d-합성곱)
        * [1D 합성곱](#1d-합성곱)
    * [자연어 처리를 위한 1D CNN](#자연어-처리를-위한-1d-cnn)
      * [Max-pooling](#max-pooling)
        * [1D CNN으로 IMDB 리뷰 분류](#1d-cnn으로-imdb-리뷰-분류)
        * [1D CNN으로 스팸 메일 분류](#1d-cnn으로-스팸-메일-분류)
        * [Multi-kernel 1D CNN으로 네이버 영화 리뷰 분류](#multi-kernel-1d-cnn으로-네이버-영화-리뷰-분류)
        * [사전 훈련된 워드 임베딩을 이용한 의도 분류](#사전-훈련된-워드-임베딩을-이용한-의도-분류)
  * [문자 임베딩](#문자-임베딩)
    * [1D CNN을 이용한 문자 임베딩](#1d-cnn을-이용한-문자-임베딩)
    * [BiLSTM을 이용한 문자 임베딩](#bilstm을-이용한-문자-임베딩)
  * [케라스를 이용한 태깅 작업](#케라스를-이용한-태깅-작업)
    * [개체명 인식](#개체명-인식)
      * [BIO 표현(Begin Inside Outside)](#bio-표현begin-inside-outside)
        * [CONLL2003](#conll2003-)
      * [양방향 LSTM으로 개체명 인식기 만들기](#양방향-lstm으로-개체명-인식기-만들기)
      * [F1 Score](#f1-score-)
      * [CRF](#crf)
      * [BiLSTM-CRF를 이용한 개체명 인식](#bilstm-crf를-이용한-개체명-인식)
      * [BiLSTM-CNN을 이툥한 개체명 인식](#bilstm-cnn을-이툥한-개체명-인식)
  * [서브워드 토크나이저](#서브워드-토크나이저)
    * [바이트 페어 인코딩](#바이트-페어-인코딩)
      * [OOV(Out Of Vocabulary)](#oovout-of-vocabulary)
      * [바이트 페어 인코딩 (Byte Pair Encoding, BPE)](#바이트-페어-인코딩-byte-pair-encoding-bpe-)
      * [자연어 처리에서의 BPE](#자연어-처리에서의-bpe-)
    * [센텐스 피스](#센텐스-피스)
    * [서브워드 텍스트 인코더](#서브워드-텍스트-인코더)
    * [허깅 인터페이스 토크나이저](#허깅-인터페이스-토크나이저)
  * [Sequence to Sequence](#sequence-to-sequence)
    * [RNN을 이용한 인코더-디코더(Seq2Seq)](#rnn을-이용한-인코더-디코더seq2seq)
    * [Seq2Seq](#seq2seq)
      * [인코더](#인코더)
      * [디코더](#디코더)
      * [Seq2Seq 디코더의 훈련과 테스트 과정의 작동 방식](#seq2seq-디코더의-훈련과-테스트-과정의-작동-방식)
        * [훈련](#훈련)
        * [테스트](#테스트)
        * [디코더 셀의 출력](#디코더-셀의-출력)
        * [교사 강요(Teacher forcing)](#교사-강요teacher-forcing)
      * [문자 레벨 기계 번역기](#문자-레벨-기계-번역기)
        * [Seq2Seq로 기계 번역기 만들기](#seq2seq로-기계-번역기-만들기)
  * [Word-level 번역기, BLEU Score](#word-level-번역기-bleu-score)
    * [BLEU Score](#bleu-score)
      * [Unigram Precision(단어 개수 카운트로 측정)](#unigram-precision단어-개수-카운트로-측정)
      * [Modified Unigram Precision(중복을 제거하여 보정하기)](#modified-unigram-precision중복을-제거하여-보정하기-)
      * [n-gram으로 확장](#n-gram으로-확장)
      * [브레버티 페널티](#브레버티-페널티)
  * [어텐션 메커니즘(Attention Mechanism)](#어텐션-메커니즘attention-mechanism)
    * [닷-프로덕트 어텐션(Dot-Product Attention)](#닷-프로덕트-어텐션dot-product-attention)
      * [어텐션 스코어를 구함](#어텐션-스코어를-구함)
      * [소프트맥스 함수를 통해 어텐션 분포(Attention Distribution)를 구함](#소프트맥스-함수를-통해-어텐션-분포attention-distribution를-구함)
      * [각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함](#각-인코더의-어텐션-가중치와-은닉-상태를-가중합하여-어텐션-값을-구함)
      * [어텐션 값과 디코더의 t 시점의 은닉 상태를 연결함](#어텐션-값과-디코더의-t-시점의-은닉-상태를-연결함)
      * [출력층 연산의 입력이 되는 s^t를 계산함](#출력층-연산의-입력이-되는-st를-계산함)
    * [바다나우 어텐션](#바다나우-어텐션)
      * [어텐션 스코어를 구함](#어텐션-스코어를-구함-1)
      * [소프트 맥스 함수를 통해 어텐션 분포를 구함](#소프트-맥스-함수를-통해-어텐션-분포를-구함)
      * [각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함](#각-인코더의-어텐션-가중치와-은닉-상태를-가중합하여-어텐션-값을-구함-1)
      * [컨텍스트 벡터로부터 s_t를 구함](#컨텍스트-벡터로부터-s_t를-구함-)
  * [트랜스포머(Transformer)](#트랜스포머transformer)
    * [트랜스포머의 주요 하이퍼 파라미터](#트랜스포머의-주요-하이퍼-파라미터)
    * [포지셔널 인코딩(Positional Encoding)](#포지셔널-인코딩positional-encoding)
    * [어텐션(Attention)](#어텐션attention)
    * [인코더(Encoder)](#인코더encoder)
      * [인코더의 셀프 어텐션](#인코더의-셀프-어텐션)
        * [Q, K, V 벡터 얻기](#q-k-v-벡터-얻기)
        * [스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)](#스케일드-닷-프로덕트-어텐션scaled-dot-product-attention)
        * [멀티 헤드 어텐션(Multi-head Attention)](#멀티-헤드-어텐션multi-head-attention)
        * [패딩 마스크](#패딩-마스크)
      * [포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)](#포지션-와이즈-피드-포워드-신경망position-wise-ffnn)
      * [잔차 연결(Residual connection)과 층 정규화(Layer Normalization)](#잔차-연결residual-connection과-층-정규화layer-normalization)
        * [잔차 연결(Residual connection)](#잔차-연결residual-connection)
        * [층 정규화(Layer Normalization)](#층-정규화layer-normalization)
      * [인코더에서 디코더로](#인코더에서-디코더로)
    * [디코더](#디코더-1)
      * [디코더의 첫 번째 서브층: Masked Multi-head self-Attention(셀프 어텐션과 룩-어헤드 마스크)](#디코더의-첫-번째-서브층-masked-multi-head-self-attention셀프-어텐션과-룩-어헤드-마스크)
      * [디코더의 두 번째 서브층: 인코더-디코더 어텐션(Encoder-Decorder Attention)](#디코더의-두-번째-서브층-인코더-디코더-어텐션encoder-decorder-attention)
    * [어텐션 정리](#어텐션-정리)
    * [학습률](#학습률)
      * [학습률 스케줄러](#학습률-스케줄러)
  * [BERT](#bert)
    * [임베딩을 사용하는 두 가지 방법](#임베딩을-사용하는-두-가지-방법)
    * [임베딩 방법의 단점](#임베딩-방법의-단점)
    * [BERT](#bert-1)
      * [BERT의 크기](#bert의-크기)
      * [BERT의 문맥을 반영한 임베딩](#bert의-문맥을-반영한-임베딩)
      * [BERT의 서브워드 토크나이저: WordPiece Embedding](#bert의-서브워드-토크나이저-wordpiece-embedding)
      * [포지션 임베딩(Position Embedding)](#포지션-임베딩position-embedding)
      * [BERT의 사전 훈련](#bert의-사전-훈련)
        * [마스크드 언어 모델](#마스크드-언어-모델)
        * [NSP](#nsp)
        * [세그먼트 임베딩(Segment Embedding)](#세그먼트-임베딩segment-embedding)
    * [BERT를 파인 튜닝(Fine-tuning)하기](#bert를-파인-튜닝fine-tuning하기)
      * [하나의 텍스트에 대한 태깅 작업](#하나의-텍스트에-대한-태깅-작업)
      * [텍스트의 쌍에 대한 분류 또는 회귀 문제](#텍스트의-쌍에-대한-분류-또는-회귀-문제)
      * [어텐션 마스크(Attention Mask)](#어텐션-마스크attention-mask)
    * [BERT의 문장 임베딩](#bert의-문장-임베딩)
  * [BERT와 GPT](#bert와-gpt)
  * [Overview](#overview)
    * [RNN](#rnn)
    * [CNN](#cnn-1)
    * [태깅](#태깅)
    * [서브워드 토크나이저](#서브워드-토크나이저-1)
    * [Seq2Seq](#seq2seq-1)
    * [BLEU](#bleu)
    * [어텐션 메커니즘](#어텐션-메커니즘)
    * [트랜스포머](#트랜스포머)
    * [BERT](#bert-2)
    * [GPT](#gpt-)
<!-- TOC -->

---
## 타이핑을 시작하며

기말고사 시험이 2주 남았다. 시험에 앞서 강의 내용을 아이패드로 필기하던 중에 애플펜슬 배터리가 방전되어,, 충전하는 동안 시간 아까워서 타이핑 하는 김에 작성해 본다.

---

## RNN을 이용한 텍스트 분류

텍스트 분류는 지도 학습이다.

- 단어에 대해 정수를 부여해야 한다.
  1. 케라스의 Embedding() 메소드는 단어 각각에 대하여 정수로 변환된 입력에 임베딩을 수행한다.
  2. _단어를 빈도순으로 정렬하여 순차적으로 정수를 부여한다._ **=> 등장 빈도수가 적은 단어를 제거할 수 있다.**



- RNN의 하이퍼 파라미터
  - hidden_units: RNN의 출력의 크기로, 은닉 상태의 크기와 같다. **=> 은닉층의 노드 개수와 같은 개념**
  - timesteps: 시점의 수로, 각 문서에서의 단어 수와 같으며, 은닉층의 개수와 같다.
  - input_dim: 입력 차원의 크기로, 임베딩 벡터의 차원과 같다. **=> 입력층의 노드** 개수



### RNN에서의 다 대 일 문제(many to one)
  - 텍스트 분류는 다 대 일 문제이다.
  - 텍스트 분류는 모든 시점에 대해서 입력을 받지만 최종 시점의 RNN 셀만이 은닉 상태를 출력, 이것이 출력층에서 활성화 함수를 통해 정답을 고르는 문제가 된다. **=> 입력은 모두, 출력은 하나!**
  - 스팸 메일 분류, LSTM으로 네이버 리뷰 감성 분류

### RNN에서의 다 대 다 문제(many to many)
  - 다 대 다 문제를 푸는 경우 양방향 LSTM을 이용한다.
  - 케라스에서는 양방향 LSTM을 사용하면서 return_sequence = False로 인자를 줄 경우 처음과 끝의 은닉 상태만을 출력으로 가지며, 이를 통해 양방향 LSTM으로 텍스트 분류를 수행할 수 있다.

### RNN으로의 분류

#### 나이브 베이즈 분류기
        텍스트 분류를 위해 전통적으로 사용되는 분류기로, 베이즈 정리를 따름



- 베이즈의 정리
```
P(A): A가 일어날 확률  
P(A|B): A가 일어난 후 B가 일어날 확률  
cf) B에 대해서도 동일  
P(A|B) = { P(B|A)*P(A) } / P(B)
```
- 스팸 메일 분류 가능


#### 이진 분류  

**활성화 함수: sigmoid**  
**손실 함수: binary_crossentropy**  

##### 스팸 메일 분류
- 이진 분류 문제이다.
    - 다 대 일 문제이다.
    - 데이터
        - v1: 스팸 여부 정보
            - ham: 0
            - spam: 1
        - v2: 메일 본문  
      
        - 정상 메일 및 스팸 메일의 _데이터가 불균형한 경우 학습 시에도 비율을 반드시 맞추어야 한다._ **=> stratif 인자 활용**
        - 훈련 데이터에 토큰화, 정수 인코딩을 진행하여야 한다.  
          cf) 정수 인코딩은 빈도수가 높을 수록 낮은 정수로 인코딩
          
##### IMDB 리뷰 감성 분류
- 이진 분류 문제이다.
    - GRU로 IMDB 감성 분류하기
        - 데이터의 크기와 길이를 제한하고 패딩을 활용
        - 임베딩 백터 차원: 100, 은닉 상태 크기: 128

    
##### 네이버 영화 리뷰 감성 분류
- 이진 분류 문제이다.
    - 과정
        1. 데이터 로드
        2. 데이터 정제
        3. 토큰화  
           cf) 형태소 분석기 Mecab 사용
        4. 정수 인코딩
        5. 빈 샘플 제거
        6. 패딩
        7. GRU LSTM으로 구현
            - 다 대 일 구조
 
    
##### BiLSTM으로 한국어 스팀 리뷰 감성 분류
- 양방향 LSTM은 두 개의 독립적인 LSTM 아키텍처를 함께 사용한다.
- 다 대 다 문제를 푸는 경우의 양방향 LSTM이다.
- 텍스트 분류 진행 시 역방향 LSTM은 마지막 time step만 본 상태로, LSTM에 유용한 정보를 가졌다고 보기 어렵다.
- 단방향 LSTM의 경우 마지막 time step에 출력이 있으면 역방향, 반대면 순방향 LSTM이다.
- 케라스에서는 양방향 LSTM을 사용하면서 return_sequence = False로 인자를 줄 경우 처음과 끝의 은닉 상태만을 출력으로 가지며, 이를 통해 양방향 LSTM으로 텍스트 분류를 수행할 수 있다.
        

#### 다중 클래스 문제  
    활성화 함수: softmax
    손실 함수: categorical_crossentropy

dense layer의 크기는 클래스 개수와 동일하게 설정    
=> **출력층의 뉴런 개수는 클래스의 개수와 같아야 한다.**   
cf) dense layer: 은닉층의 노드 개수

##### 로이터 뉴스 분류
- 로이터 뉴스 분류
    - 다중 클래스 분류 문제이다.
    - LSTM을 사용  
      cf) 과적합 방지를 위하여 EarlyStopping(monitor = 'val_loss', )를 사용할 수 있다.


---      

## 자연어 처리를 위한 합성곱 신경망

### CNN
- 이미지 처리에 좋음
 

- 다층 퍼셉트론으로 분류 시 1차원 벡터로의 변환 필요  
=> 공간 구조의 정보 유실 => CNN 사용


- 이미지는 3차원 텐서 형태  
   (h, w, ch) cf) channel을 depth라 부르기도 함.
  - 흑백 이미지: (h, w, 1) gray scale: 0 ~ 255
  - 컬러 이미지: (h, w, 3) R: 0 ~ 255, G: 0 ~ 255, B: 0 ~ 255


#### 합성곱 연산
  - 합성곱 층은 합성곱 연산으로 이미지의 특징을 추출한다.  
   kernel or filter 라는 n*m 행렬로 이미지를 훑으며 커널과 겹치는 이미지 영역을 커널 값과 모두 더한 값으로 출력하는데, 이름 **feature map(특성 맵)**이라 함.
 
##### stride
    커널의 이동 범위  

특성맵은 입력보다 작아지게 되어 가장자리 정보들이 유실될 수 있음.  
    => **zero-padding** 수행
 

##### 가중치와 바이어스
- CNN의 가중치는 커널 행렬의 원소이다.  


- 커널 내 가중치만으로 공간적 구조 정보를 보존할 수 있다.


- 커널의 영역에 있는 이미지에 커널 크기에 따른 가중치 행렬을 곱셈, 합하여 하나의 특성 맵 값을 만든다.

- 커널 적용의 결과에 편향을 더함으로써 특성 맵을 만들 수도 있다.

##### 특성 맵의 크기  
  특성 맵의 크기 O_h, O_w = (입력 데이터의 높이 - 커널의 높이)/스트라이드 크기 + 1이다.  
  패딩을 고려한다면 O_h, O_w = (입력 데이터의 높이 - 커널의 높이 + 2*패딩 크기)/스트라이드 크기 + 1이다.  

##### 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)
- 합성곱 연산의 입력은 다수의 채널을 가진 이미지 또는 특성맵이 될 수 있다.  
      cf) 단, 커널의 채널 수도 입력의 수와 동일해야 함.  
      ex) 흑백 이미지의 입력을 1채널이지만, 컬러 이미지는 3채널으로 커널도 3개여야 함.  


- 컬러 이미지에 3차원 커널을 사용하여도 특성맵의 차원은 기존과 같다.
          => 3차원 특성맵을 원한다면, 2차원 커널 3개로 이루어진 커널을 3개 만들어 주어야 함.
        
##### 풀링(Pooling)
    특성맵을 다운 샘플링하여 특성맵의 크기를 줄이는 것

일반적으로 합성곱층(합성곱 연산 + 활성화 함수) 다음에는 풀링층을 추가한다.  
풀링도 커널을 사용하지만 가중치가 없고, 채널 수에 변화를 주지 않는다.  
cf) 합성곱 연산도 채널 수 유지는 가능하다.

- 특성맵의 크기를 감소시켜 모델이 학습할 파라미터 수를 줄여 _과대적합을 방지할 수 있다._


##### 2D 합성곱
이미지에 합성곱 연산을 진행하는 것으로, 전술한 바와 같다.

##### 1D 합성곱
    자연어 처리에 사용되는 1D 합성곱에서의 입력은  
    각 단어가 벡터로 변환된 문장 행렬으로, LSTM과 동일하다.


- 문장이 토큰화, 패딩, 임베딩 층을 거치면 문장 형태의 행렬로 변환된다.
  - 행렬로 변환된 문장은 k*n의 크기를 카지는데, k: 임베딩 벡터의 차원, n: 문장의 길이이다.
  - 1D 합성곱에서 사용되는 커널의 크기는 h*k인데, k는 임베딩 벡터의 차원으로 고정되므로, 문장의 길이인 h가 곧 커널의 크기가 된다.  
    - 이때 커널의 크기 h에 따라 trigram, bigram 등의 h-gram 커널이 생성된다.

### 자연어 처리를 위한 1D CNN
#### Max-pooling
    풀링 커널 영역의 최댓값으로 풀링


- 1D CNN에서도 합성곱층 다음에는 풀링층을 추가하여야 한다.


- 이진 분류를 위한 신경망의 활성화 함수로 소프트맥스 함수를 사용한다.
 

- 커널이 stride에 따라 이동하면서 생성되는 6개의 맥스 풀링 스칼라 값을 하나의 벡터를 만들어 주며, 이는 1D CNN을 통한 문장에서 얻은 벡터이다. 이를 Dense layer에 넣어 텍스트 분류를 수행한다.

##### 1D CNN으로 IMDB 리뷰 분류
- 출력층에서의 활성화 함수: **sigmoid**
- 출력층에서의 손실 함수: **crossentropy**

##### 1D CNN으로 스팸 메일 분류
- 이진 분류
  - 출력층에 활성화 함수: **sigmoid**
  - 출력층에서의 손실 함수: **crossentropy**

##### Multi-kernel 1D CNN으로 네이버 영화 리뷰 분류

##### 사전 훈련된 워드 임베딩을 이용한 의도 분류
1. 의도 카테고리를 고유한 정수로 변환
2. Glove 임베딩 사용
3. 훈련 데이터에 있는 단어와 사전 훈련된 워드 임베딩 벡터의 값을 맵핑하여 임베딩 테이블에 저장
4. 1D CNN 이용하여 의도 분류

---

## 문자 임베딩
    단어 임베딩과 달리 단어를 문자 단위로 쪼개어 임베딩을 진행하고,  
    이를 이어 붙여 단어의 벡터를 생성


### 1D CNN을 이용한 문자 임베딩
- 1D CNN을 문자 임베딩에 사용할 경우에는 문자의 N-gram으로부터 정보를 얻을 수 있다.

### BiLSTM을 이용한 문자 임베딩
- 단어를 구성하는 각 문자를 순방향, 역방향 LSTM의 셀에 입력으로 넣고,  
    순방향 LSTM의 마지막 시점의 은닉 상태와 역방향 LSTM의 첫 시점 은닉 상태를 단어의 벡터로 사용함.
 

- 워드 임베딩의 대체제 역할을 하며, 워드 임베딩과 연결하여 함께 사용되기도 한다.

## 케라스를 이용한 태깅 작업
    토큰화 이후 각 단어가 속한 유형을 붙여 주는 것



- 태깅 작업은 텍스트 분류와 같이 **지도학습에 속함**  
    x: 태깅 대상, y: 태깅 정보
 

- 시퀀스 레이블링    
 입력 시퀀스 x에 대하여 레이블 시퀀스 y를 각각 부여하는 작업으로, 태깅은 대표적인 시퀀스 레이블링이다.
 

- 양방향 LSTM  
이전 시점이 단어 및 다음 시점의 단어 정보도 활용하기 위하여 사용된다.
 

- RNN의 다 대 다 문제
  - _태깅 작업은 다 대 다 문제이다._  cf) return_sequences = True로 설정
 

- 품사 태깅 데이터에 대한 이해와 전처리
  1. 문장 데이터 x와 품사 태깅 정보 y에 각각 토큰화
  2. 정수 인코딩

### 개체명 인식
    이름을 가진 단어(명사)를 보고 단어가 속한 유형을 인식하는 것으로,  
    개체명은 단어가 속한 유형이다.
    ex) "유정이는 2018년에 골드만삭스에 입사했다."  
    유정(이름을 가진 단어): 사람(개체명), 2018(이름을 가진 단어): 시간(개체명), 골드만삭스(이름을 가진 단어): 조직(개체명)


- NLTK에서 개체명 인식기 NER Chkunker를 지원한다.

#### BIO 표현(Begin Inside Outside)

| 문자 | BIO |   개체명   |
|:--:|:---:|:-------:|
| 해  |  B  |  movie  |
| 리  |  I  |  movie  |
| 포  |  I  |  movie  |
| 터  |  I  |  movie  |
| 보  |  O  |         |
| 러  |  O  |         |
| 메  |  B  | theater |
| 가 |  I  |    theater     |
| 박 |  I  |    theater     |
|스|  I  |    theater     |
|가|  O  |         |
|자|  O  |         |


##### CONLL2003  
개체명 인식을 위한 전통적인 영어 셋으로 [단어] [품사] [청크 태깅] [개체명 태깅]의 형식을 가짐  
ex) Peter NNP B-NP B-Per  
Blackburn NNP I-NP I-Per  
Peter Blackburn: Person

#### 양방향 LSTM으로 개체명 인식기 만들기
- 임베딩 벡터 차원: 128, 은닉 상태: 128, 다 대 다 구조
  - LSTM의 return_sequences = True => LSTM의 모든 셀의 출력을 사용  
    패딩으로 0이 많아질 경우 연산에서 제외 하기 위해 Embedding(mask_zero = True)로 설정
 

- Time_distributed()  
다 대 다 구조에서 모든 time step에 대하여 출력을 받음.

#### F1 Score  
    Precision과 Recall의 조화 평균으로, (2*P*R)/(P+R)을 따름


|   |예측 참| 예측 거짓 |
|:--:|:---:|:-----:|
|실제 참| TP |  FN   |
|실제 거짓 | FP |  TN   |


**TP: 모델이 참(P)으로 봤는데, 예측이 맞았다(T). TP => 실제 참    
FN: 모델이 거짓(N)으로 봤는데, 예측이 틀렸다(F). FN => 실제 참   
FP: 모델이 참(P)으로 봤는데, 예측이 틀렸다(F). FP => 실제 거짓  
TN: 모델이 거짓(N)으로 봤는데, 예측이 맞았다(T). TN => 실제 거짓**   

- 정밀도(Precision): 모델이 True라고 예측한 것중 실제 True인 것의 비율  
TP/(TP+FP)  
모델이 참이라고 한 것(TP+FP) 중 실제 참인 것(TP)의 비율  
 

- 재현율(Recall): 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율  
TP/(TP+FN)  
실제로 참인 것(TP+FN) 중에서 모델이 참이라고 한 것(TP)의 비율
 

- 정확도(Accuracy): 전체 예측한 데이터 중에서 정답을 맞춘 것에 대한 비율  
(TP+TN)/(TP+FN+FP+TN)  
전체 예측한 데이터 중에서 정답을 맞춘 것(TP+TN)


#### CRF
LSTM의 출력을 받아 레이블 시퀀스 중 가장 높은 점수를 가지는 시퀀스를 예측

- CRF 층은 점차적으로 훈련 데이터로부터 아래와 같은 제약사항 등을 학습함.
  - 문장의 첫 번째 단어에서는 I가 나오지 않음.
  - O-I 패턴은 나오지 않음.
  - B-I-I 패턴에서 개체명은 일관성을 유지한다. 연속된 I에서 다른 개체명이 나오면 안 됨...

즉, 양방향 LSTM은 입력 단어에 대한 양방향의 문맥을 반영하며, CRF는 출력 레이블에 대한 양방향 문맥을 반영함.


#### BiLSTM-CRF를 이용한 개체명 인식
- 문자 임베딩을 위한 전처리로, 문자 단위 정수 인코딩을 진행해야 함.
  - 단어 -> 문자 시퀀스 -> 패딩 -> 문자 단위 정수 인코딩

#### BiLSTM-CNN을 이툥한 개체명 인식
1. 하나의 단어는 문자 단위로 토큰화
2. 각 문자는 전처리를 통해 정수에 매핑
3. 각 문자가 임베딩 층을 통과
4. 1D CNN에 입력 사용하여 하나의 단어에 대한 단어 벡터를 얻음. (문자 임베딩)
5. 일반적인 단어 임베딩 벡터와 연결
6. 양방향 LSTM의 입력에 사용  
cf) 문자 임베딩의 결과는 단어 임베딩의 대체제 또는 단어 임베딩 벡터와 함께 사용될 수 있다.

---
## 서브워드 토크나이저
### 바이트 페어 인코딩
#### OOV(Out Of Vocabulary)
    세상에 모든 단어를 학습할 수 없음.  
    모델이 모르는 단어가 발생하게 되면 UNK(Unknown Token)로 표시됨.


#### 바이트 페어 인코딩 (Byte Pair Encoding, BPE)  
    연속적으로 가장 많이 등장한 글자의 쌍을 찾아 하나의 글자로 병합함.


BPE 알고리즘은 데이터 압축 알고리즘이지만, 자연어 처리의 서브워드 분리 알고리즘으로 활용됨.

ex) "aaabdaaabac"  
aa: 4회  
-> ZabdZabac, Z = aa  
-> ZYdZYac, Y = ab  
-> XdXac, X = ZY
-> 종료

#### 자연어 처리에서의 BPE  
    자연어 처리에서의 BPE는 서브워드 분리 알고리즘


- 글자 단위에서 점차적으로 단어집합을 만들어 내는 Bottom-up 방식
 

- _훈련 데이터의 모든 단어들을 문자/유니코드 단위로 단어집합을 만듦._  
    -> 빈도수가 가장 높은 유니그램을 하나의 유니그램으로 통합.
 

- 기존의 접근은 학습한 적이 없는 단어가 입력될 시 제대로 대응하지 못함. => **OOV 문제**  
 

- BPE를 사용하는 경우
  - 딕셔너리의 모든 단어들을 글자 단위로 분리함.
  - 알고리즘 반복 횟수 지정 가능. => iteration

  ex) 

```python
  ---
  # dictionary 훈련 데이터에 있는 단어와 등장 빈도수
  low: 5, lower: 2, newest: 6, widest: 3
  # vocabulary
  low, lower, newest, widest
  ---
  => 모든 단어를 글자 단위로 분리
  
    # dictionary
  l o w: 5, l o w e r: 2, n e w e s t: 6, w i d e s t: 3
  # vocabulary
    l, o, w, e, r, n, s, t, i, d
  ---
  BPE => 가장 빈도수가 높은 글자 쌍을 찾음.
  
  lo: 7, ow: 7, we: 8, er: 2, ne: 6, ew: 6, es: 9, st: 9, wi: 3, id: 3, de: 3
  -> es의 빈도수가 가장 높음

  ---
  
  => es를 하나의 글자 쌍으로 묶어서 dictionary, vocabulary 업데이트
  
  # dictionary => es 묶기
  l o w: 5, l o w e r: 2, n e w es t: 6, w i d es t: 3
  # vocabulary => es 추가
    l, o, w, e, r, n, s, t, i, d, es

  ---
  
  BPE
  
  lo: 7, ow: 7, we: 8, er: 2, ne: 6, ew: 6, est: 9, st: 9, wi: 3, id: 3, de: 3
  -> est의 빈도수가 가장 높음
  
  ---
  
  -> est를 하나의 글자 쌍으로 묶어서 다시 업데이트
  
  # dictionary => est 묶기
  l o w: 5, l o w e r: 2, n e w est: 6, w i d est: 3
  # vocabulary => est 추가
    l, o, w, e, r, n, s, t, i, d, es, est
  
  ---
  
  * * *
  
  iterator만큼 반복
  
  ---
  
  # dictionary 
  low: 5, low e r: 2, newest: 6, widest: 3
  # vocabulary
    l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest
    
  => OOV가 해결됨
```


```python
  dictionary  
  low: 5, low e r: 2, newest: 6, widest: 3  
  vocabulary  
  l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest
```


위 vocabulary를 학습한 BPE에
- 'lowing'이 입력되는 경우
  - BPE에 입력
    - iteration: 3일 때에 low, i, n, g로 묶이게 되며, vocabulary 안의 단어로 
            병합할 수 있는 글자가 없게 되어 BPE가 종료됨.  
            => low, i, n, g를 병합하여 단어 완성  

- 'highing'이 입력되는 경우
  - BPE에 입력
    - iteration: 1일 때 h, i, g, h, i, n, g로 vocabulary 안의 단어로,
    병합할 수 있는 글자가 없게 되어 BPE가 종료됨.  
    => h, i, g, h, i, n, g를 병합하여 단어 완성


- WordPiece Tokenizer  
    BPE의 변형 알고리즘으로, 병합되었을 때 코퍼스의 우도를 가장 높이는 쌍을 병합한다.


- Unigram Language Model Tokenizer
  - 각 서브 워드들의 손실을 계산한다.  
  cf) 손실: 단어 집합에서 제거되었을 때의 코퍼스 우도 감소치
  - 원하는 단어 집합 크기가 될 때까지 최악의 토큰을 10% ~ 20% 제거한다.

### 센텐스 피스
    내부 단어 문제를 위해 사용되는 패키지


### 서브워드 텍스트 인코더
    텐서플로우의 서브워드 토크나이저


### 허깅 인터페이스 토크나이저
    자주 등장하는 서브워드들을 하나의 토큰으로 취급하는 다양한 서브워드 토크나이저를 제공


---

## Sequence to Sequence
### RNN을 이용한 인코더-디코더(Seq2Seq)
기존의 RNN 구조와는 다른 RNN 인코더, RNN 디코더 두 개의 RNN을 연결하는 인코더-디코더 구조

- 입출력 문장의 길이가 다를 때 사용함. ex) 텍스트 요약 및 번역

### Seq2Seq
ex) 챗봇 - 입력: 질문, 출력: 대답  
번역 - 입력: 문장, 출력: 번역문  
내용 요약, STT 등

* 인코더와 디코더는 두 개의 RNN 아키텍처로 구성됨(LSTM과 GRU도 될 수 있음.)
 

* Seq2Seq에 사용되는 모든 단어들은 임베딩 벡터로 변환 후 입력에 사용된다.
 

* 인코더와 디코더 셀에 입력되는 데이터는 최초 시점의 인코더 셀 말고는 모두 두 개의 데이터이다.

#### 인코더
    입력 문장의 모든 단어들을 순차적으로 입력 받은 후 모든 단어 정보를 압축  
    => 컨텍스트 벡터 생성
 

- 인코더 RNN 셀은 모든 단어를 입력 받은 후 인코더 RNN 셀의 **마지막 시점 은닉 상태**(**컨텍스트 벡터**)를 디코더 RNN 셀로 전달함.
- 인코더 LSTM 셀 상위 레이어가 있다면 각 시점의 은닉 상태를 상위 레이어에 전달할 수 있다.

#### 디코더
    인코더에서 컨텍스트 벡터를 입력 받아 디코딩한 단어를 순차적으로 출력함


- 디코더는 기본적으로 RNNLM(RNN Language Model)이다.
  - 초기 입력으로 문장의 시작을 의미하는 심볼 <sos>가 들어감.
  - <eos>가 입력되면 다음에 등장할 확률이 높은 단어를 예측한다.
  - 첫 시점의 디코더 RNN 셀은 다음 시점에 올 단어를 예측하고, 이를 다음 셀의 입력으로 전달한다.
  - 문장의 끝을 의미하는 심볼 <eos>가 예측될 때까지 위 과정을 반복함.
  - Context Vector를 매 시점으 LSTM 셀에 입력할 수도 있다.  


#### Seq2Seq 디코더의 훈련과 테스트 과정의 작동 방식
##### 훈련
훈련 과정에서는 디코더에게 인코더가 보낸 컨텍스트 벡터와 실제 정답인 <sos> 문장 a 를 입력 받았을 때, 문장 a <eos>가 나오도록 정답을 알리며 훈련함

##### 테스트
디코더는 오직 컨텍스트 벡터와 <sos>만을 입력으로 받은 후 다음에 올 단어를 예측, 이를 다음 시점의 RNN 셀의 입력으로 넣는 행위를 반복한다.

##### 디코더 셀의 출력
각 디코더 셀의 은닉 상태는 **softmax**층을 거쳐 예측 단어를 출력한다.

##### 교사 강요(Teacher forcing)
훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀에 입력으로 넣어주지 않는다.  
대신 이전 시점의 실제 값을 현재 시점의 디코더 셀의 입력 값으로 사용한다.
=> _잘못 예측한 단어가 다음 시점 셀에 입력되게 되면 전체 성능이 저하해 학습 성능을 저해할 수 있기 때문이다._

#### 문자 레벨 기계 번역기
##### Seq2Seq로 기계 번역기 만들기
- 훈련 데이터로 **병렬 코퍼스**(두 개 이상의 언어가 병렬적으로 구성된 코퍼스)가 필요함.
- 교사 강요
- 심볼 - <sos>: '\t', <eos>: '\n'

---

## Word-level 번역기, BLEU Score
### BLEU Score
    기계 번역의 결과와 사람이 직접 번역한 결과가 얼마나 유사한지 비교하여 성능을 측정하는 방법


#### Unigram Precision(단어 개수 카운트로 측정)
- 유니그램 정밀도 = Ref들에 존재하는 Ca의 단어의 수 / Ca의 총 단어 수  
=> _Ref에 존재하는 단어만 반복되면 점수가 높아지는 문제가 존재함._  
ex) Ref1: "the cat is cute.", Ref2: "the cat is on the mat." Ca: "the the the the" -> 4/4 = 1

=> 중복 제거 필요

#### Modified Unigram Precision(중복을 제거하여 보정하기) 
* Max_Ref_Count: _하나의 Ref에 대한 유니그램의 최대 빈도수_  
 

* Count: Unigram Precision에서 계산하였던 유니그램의 수
 

* Count_clip: min(Count, Max_Ref_Count)
- 보정된 유니그램 정밀도: Ca의 각 Count_clip 총합 / Ca의 총 유니그램 수  
ex) Ref1: "the cat is cute.", Ref2: "the cat is on the mat." Ca1: "the the the the"   
Max_Ref_Count == 2, Count == 4, Count_clip == 2  
-> 2 / 4 = 0.5가 됨.

=> _단어의 순서가 바뀌어도 점수가 같다는 문제가 존재함._  
ex) Ref1: "the cat is cute.", Ref2: "the cat is on the mat." Ca2: "the moon is good"   

Max_Ref_Count == 2, Count = 1, Count_clip == 1    
-> 1 / 4 = 0.25  
Ca3: "moon the good is" 여도 값이 같음.

=> _순서를 고려할 수 있도록 단어를 묶어서 보자!_

#### n-gram으로 확장
n-gram 확장을 통해 단어 순서를 고려할 수 있게 됨.

=> 짧은 문장에 좋은 점수가 부여되는 문제가 존재.

=> _짧은 문장에 penalty 부여!_

#### 브레버티 페널티
    Ca가 Ref보다 짧으면 페널티 부여


```python
    if(min(len(ref1), len(ref2), *** , len(refn)) > len(Ca):
        penalty!
```

---

## 어텐션 메커니즘(Attention Mechanism)
- RNN에 기반한 Seq2Seq의 모델의 문제점
  - 고정된 크기 벡터에 모든 정보를 압축 => **정보 손실 발생**
  - 기울기 손실의 문제  
=> _매 time step마다 필요한 부분에 집중해서 전체 출력을 다시 보자!_


- 어텐션 함수  
  각 Query에 대해서 Key와의 유사도를 구해 각 Value에 반영함. 어텐션 함수의 결과는 Value의 총합(Attention Value)임.


    Q = Query: t 시점의 디코더 셀에서의 은닉 상태 
    K = Keys: 모든 시점의 인코더 셀의 은닉 상태
    V = Values: 모든 시점의 인코더 셀의 은닉 상태들

    Attention(Q, K, V) = Attention Value
 

- 어텐션 스코어
    현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점 은닉 상태 s_t와 얼마나 유사한지를 판단하는 스코어 값  
    h_i: t 시점에서 인코더의 은닉 상태    
score(s_t, h_i) = (s_t)^T * h_i
    

### 닷-프로덕트 어텐션(Dot-Product Attention)
연산 순서
1. 어텐션 스코어를 구함
2. 소프트맥스 함수를 통해 어텐션 분포를 구함
3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함
4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결함
5. 출력 연산의 입력이 되는 s^t를 계산함.

#### 어텐션 스코어를 구함
e^t = t 시점까지의 인코더 은닉 상태의 어텐션 스코어 총합

#### 소프트맥스 함수를 통해 어텐션 분포(Attention Distribution)를 구함
* 어텐션 가중치: softmax(e^t)  
어텐션 가중치 = softmax(어텐션 스코어 총합)
 

* 어텐션 분포: 어텐션 가중치의 총합(1이 되는 확률 분포임)  
    a_t = sofmax(e^t)  
    어텐션 분포 = 어텐션 가중치의 총합

#### 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함
* 어텐션 값(Attention Value): 어텐션 함수의 출력값  
어텐션 가중치와 은닉 상태를 가중합한 _어텐션 값은 인코더의 문맥을 포함_ 하고 있다고 하여 **컨텍스트 벡터**라 부름 

#### 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결함
어텐션 메커니즘은 a^t를 s^t와 결합하고 하나의 벡터로 만드는 작업을 수행함.
* v^t = a^t와 s^t의 결합  
v^t를 예측 연산의 입력으로 사용하여 인코더로부터 얻은 정보를 활용하게 됨.

#### 출력층 연산의 입력이 되는 s^t를 계산함
가중치 행렬과 곱한 후에 tanh 함수를 지나도록 하여 출력층 연산을 위한 새로운 벡터 s~^t를 얻음.  
즉, 어텐션 메커니즘에서는 출력층의 입력이 s~^t가 됨.

### 바다나우 어텐션
    Q = Query: t-1 시점의 디코더 셀에서의 은닉 상태 
    K = Keys: 모든 시점의 인코더 셀의 은닉 상태
    V = Values: 모든 시점의 인코더 셀의 은닉 상태들

    Attention(Q, K, V) = Attention Value


연산 순서 
1. 어텐션 스코어를 구함
2. 소프트 맥스 함수를 통해 어텐션 분포를 구함
3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함
4. 컨텍스트 벡터로부터 s_t를 구함

#### 어텐션 스코어를 구함
닷-프로덕트 어텐션과 달리 Query로 디코더의 t-1 시점의 은닉 상태 s_(t-1)을 사용함.


#### 소프트 맥스 함수를 통해 어텐션 분포를 구함
닷-프로덕트 어텐션과 동일


#### 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값을 구함
닷-프로덕트 어텐션과 동일


#### 컨텍스트 벡터로부터 s_t를 구함 
* LSTM은 이전 셀의 은닉 상태(s_(t-1)) + 현재 입력 (x_t)
 

* Seq2Seq는 이전 셀의 은닉 상태(s_(t-1)) + 임베딩된 단어 벡터 입력 (x_t)

---

## 트랜스포머(Transformer)
Seq2Seq 모델의 한계
- 입력 시퀀스의 벡터 압축 과정에서의 정보 손실 발생  
=> _이를 보정하기 위해 어텐션이 사용됨!_

어텐션을 RNN의 보정 용도로만 사용하지 않고, 어텐션만으로 인코더와 디코더를 만드는 것도 가능함.

트랜스포머는 RNN을 사용하지 않지만 기존의 Seq2Seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있음.

**Seq2Seq**: 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점을 가지는 구조(RNN이 인코더와 디코더를 구성)

**트랜스포머**: 인코더와 디코더라는 단위가 N개로 구성되는 구조(RNN 안 씀, 인코더와 디코더가 N개로 구성)

### 트랜스포머의 주요 하이퍼 파라미터
* 임베딩 벡터의 차원(d_model)
  - 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기
  - 각 인코더와 디코더는 입출력 차원이 유지됨.


* 트랜스포머 모델에서 인코더와 디코더의 레이어 수(num_layers)  
인코더와 디코더를 얼마나 쌓느냐에 대한 하이퍼 파라미터
 

* 병렬 어텐션의 수
  - 트랜스포머에서는 어텐션을 사용할 때, _여러 개로 분할하여 병렬으로 어텐션을 수행, 결과 값을 다시 하나로 합침_
 

* 트랜스포머 내부에는 피드 포워드 신경망(FFNN)이 존재하며 해당 신경망의 은닉층 크기를 지정할 수 있음.


### 포지셔널 인코딩(Positional Encoding)
    트랜스포머에서 단어의 위치 정보를 얻기 위해 각 단어의 임베딩 벡터에   
    위치 정보(sin, cos 두 개 함수 사용)값들을 더하는 것
 

* why to use?  
모델은 포지셔널 인코딩된 결과를 입력으로 사용함.  
RNN이 자연어 처리에서 유용한 것은 _단어의 위치에 따라 단어를 순차적으로 입력받는 특성_ 때문에 **위치 정보를 가질 수 있기 때문임**.  
반면, 트랜스포머는 단어 입력을 순차적으로 받지 않아 위치 정보를 부여할 필요가 있음.

### 어텐션(Attention)
셀프 어텐션: Query, Key, Value 벡터의 출처가 동일한 경우
트랜스포머의 인코더-디코더 어텐션: Query가 디코더의 벡터, Key, Value가 인코더의 벡터로 _셀프 어텐션이 아님..!_

 

* 인코더의 셀프 어텐션: Query = Key = Value
 

* 디코더의 마스크드 셀프 어텐션: Query = Key = Value
 

* 디코더의 인코더-디코더 어텐션: Query: 디코더 벡터 / Key = Value: 인코더 벡터
 

* 트랜스포머는 어텐션을 병렬으로 수행하기 위해 **Multi-head** 방법을 사용함.

### 인코더(Encoder)
트랜스포머는 하이퍼 파라미터인 num_layers 개수의 인코더 층을 쌓음.  
인코더를 하나의 층으로 볼 때, 하나의 인코더 층은 셀프 어텐션 층과 피드 포워드 신경망 2개의 서브층으로 나뉨.  


|                                              인코더                                              |
|:---------------------------------------------------------------------------------------------:|
| Position wise FFNN  <br/> ------------------------------------ <br/> Multi-head Self-Attention|



어텐션 함수는 주어진 Query에 대해서 모든 Key와의 유사도를 각각 구함.  
이 유사도를 가중치로 하여 키와 맵핑되어 있는 Value에 반영함.  
유사도가 반영된 Value를 모두 가중합하여 리턴함.
 

* 기존의 어텐션은 t 시점이 변화함에 따라 반복적을 쿼리를 수행하기에 전체 시점에 일반화 될 수 있다..ㅠ
 

* 셀프 어텐션은 입력 문장의 모든 단어 벡터들을 다룸으로써 _단어 간 연관성을 찾을 수 있다._ ex) 문장 내 it이 가리키는 단어가 무엇인지?


#### 인코더의 셀프 어텐션
1. Q, K, V 벡터 얻기
2. 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)


##### Q, K, V 벡터 얻기
앞에서의 셀프 어텐션과는 달리, 실제 Q, K, V 벡터는 초기 입력인 d_model의 차원을 가지는 단어 벡터보다 더 작은 차원을 가짐. (계산 효율 등의 이유)  
트랜스포머는 d_model을 num_heads로 나눈 값을 각 Q, K, V 벡터의 차원으로 가짐.  
각 단어 벡터들에 가중치 행렬을 곱하여 (d_model / num_heads)차원의 Q, K, V 벡터를 얻음. 모든 단어 벡터에 대하여 수행함. 

##### 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)
Q, K, V 벡터를 얻은 후엔 기존 어텐션 메커니즘과 동일함
1. Q 벡터는 모든 K 벡터에 대하여 어텐션 스코어를 구함
2. 어텐션 분포를 구함 
3. V 벡터에 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구함
4. 모든 Q 벡터에 반복


* 내적만을 이용하는 기존 닷-프로덕트 어텐션과는 다른 어텐션 함수를 사용하여 스케일드 닷-프로덕트 어텐션이라 함.
 

* 행렬 연산으로 어텐션 연산 일괄 처리

##### 멀티 헤드 어텐션(Multi-head Attention)
한 번의 어텐션보다 여러번의 어텐션을 병렬로 사용하는 것이 효과적

d_model의 차원을 num_heads개로 나누어 d-model / num_heads의 차원을 가지는 벡터 Q, K, V에 대해서 num_heads 개의 병렬 어텐션을 수행함  
이때 각 어텐션 값 행렬을 **어텐션 헤드**라고 부름.  
* 각 벡터에 곱해지는 _가중치 행렬의 값은 어텐션 헤드마다 전부 다르다!_

**장점**: 어텐션을 병렬으로 수행함으로써 다양한 시각에서 정보를 수집할 수 있다.  
=> _각 어텐션 헤드에서 같은 단어의 연관성을 다르게 볼 수 있기 때문!_ 

병렬 어텐션이 수행된 이후에는 모든 어텐션 헤드를 연결하는데, 그 크기는 (seq_len, d_model)

어텐션 헤드를 모두 연결한 행렬은 다른 가중치 행렬과 곱하여 최종적인 결과 행렬을 출력하며, 그 크기는 인코더의 입력이었던 문장 행렬의 크기 (seq_len, d_model)과 동일하다.  
=> 인코더의 첫번째 서브층인 멀티-헤드 어텐션을 거친 후의 출력과 입력의 크기는 같다.(인코더의 입출력은 항상 같아야 함.)  
트랜스포머는 동일한 구조의 인코더를 쌓은 구조이기에 결국 인코더 전체의 입출력은 항상 같다.

##### 패딩 마스크
어텐션 스코어 행렬의 마스킹 위치에 매우 -무한대에 가까운 음수값을 부여

`<PAD>`가 포함된 입력 문장의 셀프 어텐션 수행 과정
- 단어 <PAD>는 실질적 의미가 없는 단어임.


- 트랜스포머에서는 Key의 경우에 `<PAD>` 토큰이 존재한다면 이에 대해 유사도를 구하지 않도록 마스킹(Masking)을 함.

#### 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)
트랜스포머 인코더의 두 번째 서브층

* 포지션-와이즈 FFNN: 완전 연결 FFNN
포지션-와이즈 FFNN은 인코더와 디코더에서공통적으로 가지고 있는 서브층임.
 

* 입출력의 크기는 (seq_len, d_model)로 보존됨.

#### 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)
트랜스포머에서는 두 개의 서브층을 가진 인코더에 추가적으로 **Add & Norm** 기법을 사용함. 이는 **잔차 연결과, 층 정규화**를 의미

##### 잔차 연결(Residual connection)
    서브층의 입력과 출력을 더하는 작업으로, 모델의 학습을 도움


트랜스포머 서브층의 입출력이 동일한 차원이라 덧셈 연산이 가능함.

##### 층 정규화(Layer Normalization)
텐서의 마지막 차원에 대해서 평균과 분산을 구하고, 이를 통한 수식으로 값을 정규화하여 학습을 도움.  
cf) 텐서의 마지막 차원: d_model의 차원(입출력 크기가 같기 때문)

층 정규화의 두 가지 방법
1. 평균과 분산을 통한 정규화
2. 학습 가능한 파라미터 감마와 베타를 통한 정규화

#### 인코더에서 디코더로
구현된 인코더는 총 num_layers만큼의 층 연산을 순차적으로 진행한 후 마지막 층의 출력을 디코더에 전달함.  
디코더 역시 num_layers만큼의 층 연산을 수행 하는데, 각 디코더 층 연산에 인코더의 출력을 사용함.

### 디코더
기존의 Seq2Seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행함.  
=> _RNN을 사용하지 않지만 인코더-디코더 구조는 유지_

디코더를 하나의 층으로 볼 때, 하나의 디코더 층은 마스크드 멀티 헤드 어텐션 층과 셀프 어텐션 층, 피드 포워드 신경망 3개의 서브층으로 나뉨.


|                                                                                               디코더                                                                                               |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
| Position wise FFNN  <br/> ---------------------------------------------- <br/> Multi-head Attention <br/> ---------------------------------------------- <br/> Masked Multi-head self-Attention |



#### 디코더의 첫 번째 서브층: Masked Multi-head self-Attention(셀프 어텐션과 룩-어헤드 마스크)
* 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됨.
 

* 트랜스포머도 Seq2Seq와 같이 교사 강요(Teacher Forcing)을 사용하여 훈련함.
 

* 디코더는 <sos> 토큰이 포함된 문장 행렬을 한 번에 입력 받고 이로부터 각 시점의 단어를 예측하도록 훈련됨.

Seq2Seq의 디코더에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 입력받기 때문에 다음 단어 예측에 현재 및 이전 시점에 입력된 단어만 참고할 수 있음.  

트랜스포머는 문장 행렬로 입력을 한 번에 받기 때문에 현재 시점의 단어를 예측할 때 미래 시점의 단어까지도 참고할 수 있는 현상이 발생.
=> **룩-어헤드 마스크(Look-ahead mask)** 도입

디코더의 첫 번째 서브층인 멀티 헤드 셀프 어텐션 층은 인코더의 그것과 동일한 연산을 수행.
_오직 어텐션 스코어 행렬에 마스킹을 적용한다는 점만 다름!_

트랜스포머에는 총 세가지 어텐션이 존재함.
각 어텐션 시 함수에 전달하는 마스킹은 아래와 같음.
1. 인코더의 셀프 어텐션: 패딩 마스크 전달
2. 디코더의 마스크드 셀프 어텐션: 룩-어헤드 마스크 전달(패딩 마스크 포함)
3. 디코더의 인코더-디코더 어텐션: 패딩 마스크 전달

#### 디코더의 두 번째 서브층: 인코더-디코더 어텐션(Encoder-Decorder Attention)
디코더의 두 번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전 어텐션들과 공통점을 가짐.

셀프 어텐션은 Query, Key, Value의 출처가 같은 경우인데, 인코더-디코더 어텐션 층은 Query: 디코더 행렬, Key = Value: 인코더 행렬이라 셀프 어텐션이 아님!  
따라서 디코더의 두 번째 서브층에는 인코더 마지막 층의 결과 행렬으로부터 두 개의 입력(Key, Value)이 들어옴.  


### 어텐션 정리

|                   어텐션                   | 디코더/인코더 | 셀프 | 마스킹 적용 여부 |    어텐션 시 전달하는 마스킹     |
|:---------------------------------------:|:----------:|:--:|:---------:|:---------------------:|
|      Multi-head<br/>self-Attention      |인코더 (첫 번째 서브층) | O  |     X     |        패딩 마스크         |
|  Masked Multi-head<br/>self-Attention   |디코더 (첫 번째 서브층) |O|     O     | 룩-어헤드 마스크<br/>(패딩 마스크 포함)  |
|       Encorder-Decorder<br/>Attention        |디코더 (두 번째 서브층) |X|X |        패딩 마스크         |



### 학습률
#### 학습률 스케줄러
    미리 학습 일정을 정해두고 그 일정에 따라 학습률이 조정되는 방법


트랜스포머의 경우 사용자가 정한 단계까지 학습률을 증가, 점차 낮춰가는 방식임.

---

## BERT
### 임베딩을 사용하는 두 가지 방법
1. 임베딩 층을 랜덤 초기화하여 처음부터 학습
2. 학습된 임베딩 벡터들을 가져와 사용(데이터가 적은 경우 유리)

### 임베딩 방법의 단점
두 가지 방법 모두 하나의 단어가 하나의 벡터값으로 맵핑, 문맥을 고려하지 못함.  
=> 다의어나 동음이의어를 구분하지 못함.

* Semi-supervised Sequence Learning  
레이블이 없는 데이터로 학습된 LSTM과 가중치가 랜덤으로 초기화된 LSTM 비교 시 사전 훈련된 언어 모델을 사용한 전자가 더 좋은 성능

* ELMo  
순방향 언어 모델과 역방향 언어 모델을 따로 학습 시킨 후 사전 학습된 언어 모델로 임베딩 값을 얻음.  
=> 문맥에 따라서 임베딩 벡터가 달라져 다의어를 구분할 수 없었던 문제 해결 가능.  

* GPT-1   
트랜스포머의 디코더는 순차적으로 이전 단어들로부터 다음 단어를 예측

* 양방향 모델  
이미 예측해야 하는 단어를 역방향 언어 모델을 통해 미리 관측한 셈이므로 언어 모델은 일반적으로 양방향으로 구현하지 않음.    
하지만 언어의 문맥은 실제로 양방향이므로, 그 대안으로 ELMo에서는 순방향과 역방향 두 개의 단방향 언어 모델을 따로 준비하여 학습.    
=> 새로운 구조의 언어 모델 마스크드 언어 모델 탄생

* 마스크드 언어 모델  
입력 텍스트의 단어 집합의 15% 단어를 랜덤으로 마스킹함.   
문장에 들어갈 마스킹된 단어를 예측하도록 학습  

### BERT
레이블이 없는 방대한 데이터로 사전 훈련된 모델을 가지고, 레이블이 있는 다른 작업에 대해 추가 훈련을 하여 하이퍼 파라미터를 재조정하였음.  
=> **파인 튜닝**  

사전 학습된 BERT 모델 위에 신경망을 추가하여 파인튜닝함.

#### BERT의 크기
트랜스포머의 인코더를 쌓아올린 구조
초기 트랜스포머에 비하여 훨씬 큰 모델임.

#### BERT의 문맥을 반영한 임베딩
새로운 문장이 시작되면 [CLS]로 시작함.  

BERT의 연산을 거친 후 출력 임베딩은 문장의 문맥을 모두 참고한 문맥을 반영한 임베딩이 됨.

[CLS]라는 벡터는 BERT의 초기 입력으로 사용되었을 때에는 단순히 임베딩 층을 지난 임베딩 벡터지만, BERT를 지나고 나면 모든 단어 벡터들을 모두 참고한 문맥 정보를 가진 벡터가 됨.

하나의 단어가 모든 단어를 참고하는 연산은 BERT의 12개 층에서 모두 이루어짐.

=> **셀프 어텐션**을 통해 가능한 작업

BERT는 각 층마다 헤드 셀프 어텐션과 포지션 와이즈 피 포워드 신경망을 수행함.


#### BERT의 서브워드 토크나이저: WordPiece Embedding
BERT는 단어보다 더작은 단위로 쪼개는 서브워드 토크나이저, **WordPiece**를 사용  
WordPiece 토크나이저는 BPE와 유사하게 글자로부터 서브워드들을 병합해 가는 방식으로 Vocabulary를 만듦.

_자주 등장하지 않는 단어는 더 작은 단위인 서브워드로 분리, 서브워드들이 Vocabulary에 추가되는 아이디어_


토큰화 과정
1. 토큰이 단어 집합(Vocabulary)에 존재함.  
=> 토큰 분리 X


2. 토큰이 단어 집합(Vocabulary)에 존재하지 않음.  
=> 해당 토큰을 서브워드로 분리  
=> 해당 토큰의 첫 번째 서브워드를 제외한 나머지 서브워드들은 앞에 "##"을 붙여서 토큰으로 가짐.


=> 기존의 서브워드 토크나이저에서 OOV 문제를 발생 시키는 단어들을 서브워드로 처리할 수 있음.

#### 포지션 임베딩(Position Embedding)
트랜스포머에서는 포지셔널 인코딩이라는 방법을 통해서 단어의 위치 정보를 표현함.  

cf) 포지셔널 인코딩: 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 다른 벡터들과 더하는 방법  

BERT에서는 위치 정보를 사인 함수와 코사인 함수가 아닌 _학습을 통해 얻을 수 있는 포지션 임베딩이라는 방법을 사용._

WordPice Embedding을 통해 얻은 입력에 포지션 임베딩을 통해 위치 정보를 더함.

위치 정보를 위한 임베딩 층을 하나 사용하여 문장의 길이와 같은 개수의 포지션 임베딩 벡터를 학습, BERT의 입력마다 포지션 임베딩 벡터를 더함.  
ex) 첫 번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터, 두 번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터

실제 BERT에서는 문장의 길이를 512로 하고 있으므로, 총 512개의 포지션 임베딩 벡터가 학습됨.


BERT에서는 WordPice Embedding, Position Embedding, Segment Embedding 세 개의 임베딩 층을 사용함.


#### BERT의 사전 훈련
1. 마스크드 언어 모델
2. 다음 문장 예측(NSP)

##### 마스크드 언어 모델
전부 마스크로 변경 X  
랜덤으로 선택된 15% 단어들은, 마스크, 랜덤 단어 변경, 원문 유지(BERT는 바뀐 건지 아닌지 모름) 등의 규칙이 적용되어 마스킹됨.

##### NSP
BERT는 두 개의 문장을 제공 받고 이어지는 문장인지에 대한 여부를 학습한다.  
cf) 1:1 비율로 실제 이어지는 두 개의 문장과 랜덤으로 이어붙인 두 개의 문장으로 훈련함.  
BERT의 입력으로 넣을 때에는 첫 번째 문장 끝에 [SEP] 토큰을 넣어 문장을 구분함. 두 번째 문장도 동일.  
두 문장이 실제 이어지는지를 [CLS] 토큰의 위치의 출력층에서 이진 분류 문제를 풀도록 함.([CLS] 토큰은 BERT가 분류 문제를 풀기 위해 추가된 특별 토큰)  


##### 세그먼트 임베딩(Segment Embedding)
BERT는 문장 구분을 위해서 세그먼트 임베딩 층을 사용함.  
두 문장을 구분하는 두 개의 임베딩 벡터만 사용됨.

QA 문제의 경우 질문과 본문 두 가지로, 두 종류의 텍스트를 입력 받아 모두 두 종류의 텍스트/문서가 사용될 수 있지만,  
감성 분류와 같은 작업을 수행할 때에는 하나의 문서만 사용되므로, BERT의 입력 전체에 Sentence 0 임베딩만 더해 주게 됨.  


### BERT를 파인 튜닝(Fine-tuning)하기
#### 하나의 텍스트에 대한 태깅 작업
BERT를 사용하는 두 번째 유형은 태깅 작업.  
출력층에서는 입력 텍스트의 각 토큰 위치에 밀집층을 사용하여 분류에 대한 예측 수행

#### 텍스트의 쌍에 대한 분류 또는 회귀 문제
BERT는 자연어 추론 등의 경우와 같이 텍스트의 쌍을 입력으로 받는 태스크도 풀 수 있음.  
* 자연어 추론 문제: 두 문장이 주어졌을 때, 하나의 문장이 다른 문장과의 논리적인 관계를 분류하는 문제 ex) 모순 관계 등  
이 경우 Sentence 0, Sentence 1 임베딩 모두 사용하여 문서를 구분


#### 어텐션 마스크(Attention Mask)
    BERT가 어텐션 연산을 할 때, 불필요하게 패딩 토큰에 대해 어텐션을 수행하지 않도록 실제 단어와 패딩 토큰을 구분할 수 있도록 하는 입력
    1: 실제 단어, 0: 패딩 토큰



### BERT의 문장 임베딩
사전 학습된 BERT로부터 문장 벡터를 얻는 방법은 세 가지가 있음.
1. BERT의 [CLS] 토큰의 출력 벡터를 문장 벡터로 간주함.
2. BERT의 모든 단어의 출력 벡터에 대해서 평균 풀링을 수행한 벡터를 문장 벡터로 간주함.
3. BERT의 모든 단어의 출력 벡터에 대해서 맥스 풀링을 수행한 벡터를 문장 벡터로 간주함.

* 평균 풀링: 모든 단어의 의미를 반영함.
* 맥스 풀링: 중요한 단어의 의미를 반영함.

## BERT와 GPT
GPT는 코퍼스만을 가지고 사전학습됨. 다음에 올 토큰을 예측하는 방식.
* BERT
  * _트랜스포머의 인코더 사용_
  * 파인 튜닝 사용

* GPT
  * _트랜스포머의 디코더 사용_
* GPT-1
  * 파인 튜닝 사용
* GPT-2, 3
  * 파인 튜닝 사용 안 함
  * In-context learning 사용


## Overview
중점적으로 보기.
### RNN
- 나이브 베이즈 정리  
[나이브 베이즈 분류기 바로가기](#나이브-베이즈-분류기)

### CNN
- 1D CNN  
[1D CNN 바로가기](#자연어-처리를-위한-1d-cnn)


- 문자 임베딩  
[문자 임베딩 바로가기](#문자-임베딩)


### 태깅
[태깅 품사 바로 가기](#케라스를-이용한-태깅-작업)  
- 품사 태깅
  - 다 대 다 문제
- BIO 표현  
[BIO 표현(Begin Inside Outside) 바로가기](#bio-표현begin-inside-outside)  

- F1 Score  
[F1 Score 바로 가기](#f1-score-)


- CRF  
[CRF 바로가기](#crf)

### 서브워드 토크나이저
- BPE 정의 중요  
[BPE 바로가기](#바이트-페어-인코딩)


- 센텐스피스  
[센텐스 피스 바로가기](#센텐스-피스)


### Seq2Seq
[Seq2Seq 바로가기](#seq2seq)

### BLEU
[BLEU Score 바로가기](#bleu-score)


### 어텐션 메커니즘
계산 중요
[어텐션 메커니즘(Attention Mechanism) 바로가기](#어텐션-메커니즘attention-mechanism)


### 트랜스포머
[트랜스포머(Transformer) 바로가기](#트랜스포머transformer)
구조도 잘 보기
- 룩-어헤드 마스크
  [디코더의 첫 번째 서브층: Masked Multi-head self-Attention(셀프 어텐션과 룩-어헤드 마스크) 바로 가기](#디코더의-첫-번째-서브층-masked-multi-head-self-attention셀프-어텐션과-룩-어헤드-마스크)


### BERT
[BERT 바로가기](#bert)
- 마스크드 언어 모델  
[마스크드 언어 모델 바로가기](#마스크드-언어-모델)


- 세그먼트 임베딩
[세그먼트 임베딩(Segment Embedding) 바로가기](#세그먼트-임베딩segment-embedding)



### GPT 
[BERT와 GPT 바로가기](#bert와-gpt)
