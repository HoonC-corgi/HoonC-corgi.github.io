---
published: True
use_math: true
---

# **딥러닝과 자연어처리 타이핑**

-----

## 타이핑을 시작하며

기말고사 시험이 2주 남았다. 시험에 앞서 강의 내용을 아이패드로 필기하던 중에 애플펜슬 배터리가 방전되어,, 충전하는 동안 시간 아까워서 타이핑 하는 김에 작성해 본다.

---

## RNN을 이용한 텍스트 분류

텍스트 분류는 지도 학습이다.

- 단어에 대해 정수를 부여해야 한다.
  1. 케라스의 Embedding() 메소드는 단어 각각에 대하여 정수로 변환된 입력에 임베딩을 수행한다.
  2. _단어를 빈도순으로 정렬하여 순차적으로 정수를 부여한다._ **=> 등장 빈도수가 적은 단어를 제거할 수 있다.**



- RNN의 하이퍼 파라미터
  - hidden_units: RNN의 출력의 크기로, 은닉 상태의 크기와 같다. **=> 은닉층의 노드 개수와 같은 개념**
  - timesteps: 시점의 수로, 각 문서에서의 단어 수와 같으며, 은닉층의 개수와 같다.
  - input_dim: 입력 차원의 크기로, 임베딩 벡터의 차원과 같다. **=> 입력층의 노드** 개수



### RNN에서의 다 대 일 문제(many to one)
  - 텍스트 분류는 다 대 일 문제이다.
  - 텍스트 분류는 모든 시점에 대해서 입력을 받지만 최종 시점의 RNN 셀만이 은닉 상태를 출력, 이것이 출력층에서 활성화 함수를 통해 정답을 고르는 문제가 된다. **=> 입력은 모두, 출력은 하나!**
  - 스팸 메일 분류, LSTM으로 네이버 리뷰 감성 분류

### RNN에서의 다 대 다 문제(many to many)
  - 다 대 다 문제를 푸는 경우 양방향 LSTM을 이용한다.
  - 케라스에서는 양방향 LSTM을 사용하면서 return_sequence = False로 인자를 줄 경우 처음과 끝의 은닉 상태만을 출력으로 가지며, 이를 통해 양방향 LSTM으로 텍스트 분류를 수행할 수 있다.

### RNN으로의 분류
#### 이진 분류
**활성화 함수: sigmoid**  
**손실 함수: binary_crossentropy**

##### 스팸 메일 분류
- 이진 분류 문제이다.
    - 다 대 일 문제이다.
    - 데이터
        - v1: 스팸 여부 정보
            - ham: 0
            - spam: 1
        - v2: 메일 본문
        - 이외의 내용은 불필요하다.
        - 정상 메일 및 스팸 메일의 _데이터가 불균형한 경우 학습 시에도 비율을 반드시 맞추어야 한다._ **=> stratif 인자 활용**
        - 훈련 데이터에 토큰화, 정수 인코딩을 진행하여야 한다.  
          cf) 정수 인코딩은 빈도수가 높을 수록 낮은 정수로 인코딩
- 나이브 베이즈 분류기
  > 텍스트 분류를 위해 전통적으로 사용되는 분류기로 베이즈 정리를 따름
    - 베이즈의 정리
$$
          P(A): A가 일어날 확률 \\ 
          P(A|B): A가 일어난 후 B가 일어날 확률 \\
          cf) B에 대해서도 동일 \\
          P(A|B) = { P(B|A)*P(A) } / P(B) \\
$$
      - 스팸 메일 분류 가능

##### IMDB 리뷰 감성 분류
- 이진 분류 문제이다.
    - GRU로 IMDB 감성 분류하기
        - 데이터의 크기와 길이를 제한하고 패딩을 활용
        - 임베딩 백터 차원: 100, 은닉 상태 크기: 128

    
##### 네이버 영화 리뷰 감성 분류
- 이진 분류 문제이다.
    - 과정
        1. 데이터 로드
        2. 데이터 정제
        3. 토큰화  
           cf) 형태소 분석기 Mecab 사용
        4. 정수 인코딩
        5. 빈 샘플 제거
        6. 패딩
        7. GRU LSTM으로 구현
            - 다 대 일 구조
 
##### BiLSTM으로 한국어 스팀 리뷰 감성 분류
- 양방향 LSTM은 두 개의 독립적인 LSTM 아키텍처를 함께 사용한다.
- 다 대 다 문제를 푸는 경우의 양방향 LSTM이다.
- 텍스트 분류 진행 시 역방향 LSTM은 마지막 time step만 본 상태로, LSTM에 유용한 정보를 가졌다고 보기 어렵다.
- 단방향 LSTM의 경우 마지막 time step에 출력이 있으면 역방향, 반대면 순방향 LSTM이다.
- 케라스에서는 양방향 LSTM을 사용하면서 return_sequence = False로 인자를 줄 경우 처음과 끝의 은닉 상태만을 출력으로 가지며, 이를 통해 양방향 LSTM으로 텍스트 분류를 수행할 수 있다.
        

#### 다중 클래스 문제  
    활성화 함수: softmax
    손실 함수: categorical_crossentropy
dense layer의 크기는 클래스 개수와 동일하게 설정 **=> 출력층의 뉴런 개수는 클래스의 개수와 같아야 한다.**  
cf) dense layer: 은닉층의 노드 개수

##### 로이터 뉴스 분류
- 로이터 뉴스 분류
    - 다중 클래스 분류 문제이다.
    - LSTM을 사용  
      cf) 과적합 방지를 위하여 EarlyStopping(monitor = 'val_loss', )를 사용할 수 있다.

      