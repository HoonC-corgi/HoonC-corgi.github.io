<u>***Temp Posting***</u>  

---
published: True
---

# **YOLOv8 Concrete Crack Detection using Instance Segmentation #2**

-----

## Review


현장실습이 바빠 한동안 글을 쓰지 못했다. 오늘은 지난 게시글의 연장선을 이어 가려 한다.

[이전 게시글](https://hoonc-corgi.github.io/2023/11/24/YOLOv8-콘크리트-크랙-탐지.html)

지난 글에서는 학습을 위한 기본적인 환경 세팅을 진행하였다. 이번 게시글에서는 구체적인 학습 환경, 학습 결과, 모델을 이용한 예측을 개괄적으로 살펴 볼 것이다.  
텍스트 ai만 잠깐 공부하고는 급하게 시작한 비전 ai 프로젝트였기 때문에 비전 ai에 대한 근본적인 개념에 대한 이해와 내가 필요로 하는 버전의 YOLO 사용법을 함께 공부해야 했고, 개념을 혼동한다던가, 레퍼런스를 잘못 찾아 5 메소드를 3에 사용하면서 왜 안 되나,, 좌절하고는 했다. ~~되는 게 이상하지 ㅋㅋ~~  


기본적으로 YOLO를 쓸 수 있겠다 싶을 때에는 가령, 버전마다 상이한 백본 구조나 메소드 구조, 입력 층의 요구 차원들에 대한 이해, backward compaitibility에 막히는 등 YOLOv3부터 v5, v8으로 오기까지 정말 많은 고생을 했다.  
하지만 막히고 다른 방법으로 시도하는 것을 이과정에서 반복하며 체득했기 때문에, 높은 수준은 아님에도 현재 정도의 이해도를 갖게 되듯 하다.   
이것이야 말로 이 블로그에 쓰면 좋을 이야기들이지만, 당시에는 문서 형태의 기록을 남긴다 거나, 과정 중의 사진을 찍어 놓지는 않았기에 이를 일일이 기억하며 쓰기는 불가능할 것 같다.    
p.s. 아래는 내 노력의 흔적,,  

![노력의 흔적](/Users/gimseonghun/HoonC-corgi.github.io/posting_images/2024-01-18/노력의 흔적.png)
<img src="/Users/gimseonghun/HoonC-corgi.github.io/posting_images/2024-01-18/노력의 흔적.png"/>


여튼 본문을 시작해 보겠다.


---

ai 모델 학습에서 필수적인 요소는 아래와 같다.

1. 모델
2. 학습 데이터

나도 처음에는 간단하다고 생각했다. 하지만, 프로젝트의 완성에 다가온 현재 시점에서 생각해 보면 이는 지나치게 포괄적이다.  
실제로 모델을 활용하기 위해서는  

**1. 나의 상황에 적절한 모델이 무엇인가를 찾아야 하고,**   
**2. 어떻게 사용해야 하는지를 알아야 하며,**   
**3. 어떠한 구조인지를 정확히 이해해야만 한다.**  


학습 데이터는 train, valid, test 셋에서의 각 비율을 몇으로 둘 것인지부터가 시작인 것 같지만, 나는 대부분의 시간을 이가 아닌 학습 데이터의 확보 및 정제, 레벨링에서 써야만 했다.  
이렇게 학습이 완료되면, 학습한 모델을 어떻게 사용할 것인지에 대한 application 영역에 다가가지만, 이 단계에서도 복잡한 여러 과정을 거쳐야만 하므로 이부분은 추가적인 글을 통해 작성해 보겠다.

## 모델 학습
#### 모델


<img src="/Users/gimseonghun/HoonC-corgi.github.io/posting_images/2024-01-18/hololens2.png"/>


나는 초기 프로젝트 계획에 따라 Microsoft사의 AR Machine인 HoloLens2에서 이를 구동하길 원했기 때문에 실시간성이 중요했고, 이를 기준으로 모델을 찾아 보았다.  
개중 ultralytics사의 YOLO 모델은 객체 탐지가 빨라 실시간성을 충족하여 부상한 비전 모델으로, YOLO 안에서도 모델의 버전과, 용도에 따른 규모의 차이, 기능인 'Task'에 따라 여러 종류로 나뉘었고, 나는 YOLOv3, v5, v8의 Object Detection, Segment Instance, nano, x 모델 등을 직접 사용해 본 후 활용성, 합목적성, 난이도 등을 고려하여 최종적으로 _**YOLOv8n-seg**_ 모델을 선택하였다. 



내가 사용한 모델의 이름을 풀어보면, YOLO: 모델 이름, v8: 8버전, n: nano 버전, seg: Instance Segmentation Task를 수행하는 모델임을 의미한다. YOLO 모델 자체에 대한 자세한 사항은 추후에 공부하는 게시글을 정리해서 올리게 되면 다뤄보도록 하겠다.

#### 학습 데이터
###### 데이터 수집

최초에는 학습 데이터를 마련하기 위해 직접 사진을 찍으러 다녔다. 건물 지하 주차장의 외벽, 주변의 구축 건물, 현장실습으로 다니는 회사의 벽, 참 많은 곳을 둘러다녔다.  
길을 걸어갈 때에는 보이던 균열들이 막상 찾아보니 많지 않아 고민 하던 중 학과 건물 바로 뒤에 위치한 전기관에 가서 홀로 나름의 데이터 선별을 거쳐 사진을 찍어 오니 28장 쯤 되었다.  
아무리 비전 ai가 처음이었던 나라고 하여도, 이정도 데이터로는 택도 없음을 알고 있었다. 그렇게 잔머리를 굴리던 중 ***"사진을 45도 정도만 기울여도 다른 데이터가 아닌가?"*** 싶은 생각이 들었다.  
아무리 AI라고 한들, 이는 지도 학습이었고, 내가 제공하는 사진을 스스로 기울여 가며 학습을 하는 것은 아니기에 가능하다고 생각했다.  
그렇게 우연히 비전 ai의 **Data preprocessing, 데이터 전처리** 기법, **Data Augmentations, 데이터 증강** 기법에 관심을 가지게 되었다.  
처음에는 이것의 활용 여부는 선택이라 생각했지만, 나와 같이 데이터가 부족한 경우나 데이터를 정제해야 할 때 이는 필수적임을 알게 되었다.  
"그러한 경우가 아니라면 선택의 영역인가?" 라고 생각할 수 있지만, 모델의 퍼포먼스를 위해서라도 <u>***대부분의 학습에서 데이터 정제는 필수적이다.***</u> 


<img src="/Users/gimseonghun/HoonC-corgi.github.io/posting_images/2024-01-18/증강기법 적용 사진.png"/>  

나는 그렇게 28장의 사진에 수없이 증강, 전처리 기법을 적용하고 roboflow의 기능을 통해 임의 학습 시키며, 현재 나의 상황에 최적이라 판단 되는 조합을 찾았다.  

<img src="/Users/gimseonghun/HoonC-corgi.github.io/posting_images/2024-01-18/data preprocessing.png"/>

- **Preprocessing-Resize**  
YOLOv8의 경우 640, 640의 크기를 기대하기 때문에, Resize 해주었다.


- **Augmentations**  
  - **Flip**  
  데이터를 수집할 때 습관적으로 정방향에 수평을 맞추어 찍은 탓에 수평 수직 대칭을 해 주었다.
  + **Crop**  
  전기관 벽면 데이터를 수집함에 있어서 높이 때문에 정확히 촬영하기 어려운 경우가 있어, 축소하지는 않고, 확대한 사진도 추가해 주었다.
  - **Exposure**  
  전기관의 경우 벽면이 회색인 경우가 많았고, 크랙은 검은색이 대부분이었기에, 노출 값을 조정해 주어 해당 경우에 대한 과적합을 줄일 수 있을 것으로 보았다.
  + **Noise**  
  노이즈의 경우, 라벨링 된 가장자리의 인접 픽셀에 의도적으로 노이즈를 주어, 학습 후에 보다 정밀한 Masking에 도움이 된다. 특히 균열의 경우 규칙적이거나, 선형적이지 않은 가장자리가 많으므로 도움이 될 것으로 보았다.
  - **Mosaic**  
  모자이크는 하나의 이미지를 여러 개의 작은 타일로 조각내어 이를 무작위 재조합하여 학습함으로써 다양한 위치와, 배경, 조명 등의 변수에 대해 안정성을 가질 수 있도록 한다. 다양한 변수들을 학습하기 힘든 소규모 데이터 셋에서 특히 더 큰 효과를 내므로, 이는 나에게 최적이었다.

  